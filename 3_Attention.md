[3_Attention.py](./3_Attention.py)


多头注意力机制实现步骤：


**1. 线性投影：** 将查询、键和值矩阵分别线性变换投影到多个不同的子空间。

**2. 缩放点积：** 计算Q和K的点积，然后除以缩放因子。有助于稳定梯度并防止注意力权重变得太大。

**3. Softmax：** 对缩放后的点积应用 softmax 函数，得到注意力权重。

**4. 加权求和：** 将注意力权重与V相乘，得到加权和。